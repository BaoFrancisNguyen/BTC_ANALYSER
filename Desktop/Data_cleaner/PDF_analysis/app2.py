import streamlit as st
import pandas as pd
import os
import tempfile
import matplotlib.pyplot as plt
import seaborn as sns
import datetime
import hashlib
import io
import base64
from data_transformer_ollama2 import DataTransformer
from analysis_history2 import AnalysisHistory
from pdf_processor import PDFProcessor  # Notre nouveau module
from pdf_analysis_history import PDFAnalysisHistory  # Extension pour l'historique PDF
import logging
from add_graphs import create_charts


# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# Set page configuration
st.set_page_config(
    page_title="Data Transformation avec PDF & M√©moire",
    page_icon="C:\\Users\\Francis\\Desktop\\Data_cleaner\\PDF_analysis\\orbit-icon-13.jpg",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Initialiser l'historique des analyses CSV
@st.cache_resource
def get_analysis_history():
    """Create or retrieve the analysis history manager"""
    return AnalysisHistory(storage_dir="./analysis_history")

# Initialiser l'historique des PDF
@st.cache_resource
def get_pdf_history():
    """Create or retrieve the PDF analysis history manager"""
    return PDFAnalysisHistory(storage_dir="./analysis_history/pdf")

# Fonction pour calculer un hash de dataframe pour identifier les datasets
def get_dataframe_hash(df):
    """Generate a hash for a dataframe to identify it"""
    column_str = "_".join(df.columns)
    shape_str = f"{df.shape[0]}_{df.shape[1]}"
    sample_data = df.head(5).to_json()
    
    hash_input = column_str + shape_str + sample_data
    return hashlib.md5(hash_input.encode()).hexdigest()

def create_download_link(df, filename):
    """Create a download link for dataframe"""
    csv = df.to_csv(index=False)
    return f'<a href="data:file/csv;base64,{base64.b64encode(csv.encode()).decode()}" download="{filename}">T√©l√©charger {filename}</a>'

def create_text_download_link(text, filename):
    """Cr√©e un lien de t√©l√©chargement pour un texte format√©"""
    b64 = base64.b64encode(text.encode()).decode()
    return f'<a href="data:file/txt;base64,{b64}" download="{filename}">T√©l√©charger {filename}</a>'

def plot_correlations(df):
    """Create correlation plot for numeric columns"""
    numeric_cols = df.select_dtypes(include=['number']).columns
    if len(numeric_cols) > 1:
        corr = df[numeric_cols].corr()
        plt.figure(figsize=(10, 8))
        sns_plot = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
        return plt
    return None

def plot_missing_values(df):
    """Create missing values visualization"""
    missing = df.isna().sum().sort_values(ascending=False)
    missing = missing[missing > 0]
    if len(missing) > 0:
        plt.figure(figsize=(10, 6))
        missing.plot(kind='bar')
        plt.title('Missing Values by Column')
        plt.ylabel('Count')
        plt.xlabel('Columns')
        plt.xticks(rotation=45)
        plt.tight_layout()
        return plt
    return None

def get_dataset_info(df):
    """Extract basic information about the dataset"""
    dataset_info = {
        "dimensions": df.shape,
        "columns": list(df.columns),
        "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()},
        "columns_types": {
            "numeric": list(df.select_dtypes(include=['number']).columns),
            "categorical": list(df.select_dtypes(exclude=['number']).columns)
        },
        "missing_values": df.isna().sum().sum(),
        "duplicate_rows": df.duplicated().sum()
    }
    return dataset_info

def get_combined_context(dataset_context=None, pdf_context=None, max_analyses=3):
    """Combine les contextes d'analyse de datasets et de PDF"""
    history = get_analysis_history()
    pdf_history = get_pdf_history()
    
    logger.info(f"Chemin d'historique CSV: {history.storage_dir}")
    logger.info(f"Chemin d'historique PDF: {pdf_history.storage_dir}")
    
    # R√©cup√©rer le contexte des datasets si demand√©
    csv_context = ""
    if dataset_context is not None:
        csv_context = history.generate_context(
            dataset_name=dataset_context, 
            max_analyses=max_analyses
        )
        logger.info(f"Contexte CSV g√©n√©r√©: {len(csv_context)} caract√®res")
    
    # R√©cup√©rer le contexte des PDF
    pdf_history_context = ""
    # M√™me si pdf_context est None, r√©cup√©rons les analyses PDF r√©centes
    pdf_analyses = pdf_history.get_recent_pdf_analyses(limit=max_analyses)
    logger.info(f"Nombre d'analyses PDF r√©centes trouv√©es: {len(pdf_analyses)}")
    
    if pdf_analyses:
        if pdf_context:
            # Si un PDF sp√©cifique est demand√©
            pdf_history_context = pdf_history.generate_pdf_context(
                pdf_id=pdf_context,
                max_analyses=max_analyses
            )
        else:
            # Sinon utiliser les analyses r√©centes
            pdf_history_context = pdf_history.generate_pdf_context(
                max_analyses=max_analyses
            )
        logger.info(f"Contexte PDF g√©n√©r√©: {len(pdf_history_context)} caract√®res")
    else:
        logger.info("Aucune analyse PDF disponible pour g√©n√©rer un contexte")
    
    # Combiner les contextes
    combined_context = ""
    if csv_context and pdf_history_context:
        combined_context = f"{csv_context}\n\n{pdf_history_context}"
        logger.info(f"Contexte combin√© (CSV+PDF): {len(combined_context)} caract√®res")
    else:
        combined_context = csv_context or pdf_history_context
        if csv_context:
            logger.info(f"Contexte CSV uniquement: {len(csv_context)} caract√®res")
        elif pdf_history_context:
            logger.info(f"Contexte PDF uniquement: {len(pdf_history_context)} caract√®res")
        else:
            logger.info("Aucun contexte g√©n√©r√©")
    
    return combined_context

def process_csv_data():
    """Fonction pour traiter les donn√©es CSV"""
    # Initialiser l'historique des analyses
    history = get_analysis_history()
    
    st.header("üìà Analyse de donn√©es CSV")
    uploaded_file = st.file_uploader("Choisir un fichier CSV", type="csv", key="csv_uploader")
    
    if uploaded_file is not None:
        # Load the data
        try:
            df = pd.read_csv(uploaded_file)
            st.success(f"Donn√©es charg√©es avec succ√®s : {df.shape[0]} lignes, {df.shape[1]} colonnes")
            
            # Generate a unique identifier for this dataset
            df_hash = get_dataframe_hash(df)
            dataset_name = uploaded_file.name
            
            # Show data preview
            with st.expander("Aper√ßu des donn√©es", expanded=True):
                st.dataframe(df.head())
            
            # Apr√®s l'expander "Aper√ßu des donn√©es"
            with st.expander("Visualisations interactives", expanded=False):
                # Enregistrer le dataframe dans session_state pour create_charts
                st.session_state["df"] = df
                # Appeler la fonction de cr√©ation de graphiques
                create_charts()
                
                # Show basic statistics
                st.subheader("R√©sum√© des donn√©es")
                col1, col2 = st.columns(2)
                with col1:
                    st.write("Colonnes num√©riques :", len(df.select_dtypes(include=['number']).columns))
                    st.write("Colonnes cat√©gorielles :", len(df.select_dtypes(exclude=['number']).columns))
                with col2:
                    st.write("Valeurs manquantes :", df.isna().sum().sum())
                    st.write("Lignes dupliqu√©es :", df.duplicated().sum())
                    
                # Plot missing values
                missing_plot = plot_missing_values(df)
                if missing_plot:
                    st.subheader("Distribution des valeurs manquantes")
                    st.pyplot(missing_plot)
            
            # Try to find similar datasets in history
            similar_dataset = None
            if st.session_state.get("use_history", True):
                similar_dataset = history.find_dataset_by_similarity(df)
                if similar_dataset:
                    st.info(f"üìå Ce jeu de donn√©es semble similaire √† '{similar_dataset}' que vous avez d√©j√† analys√©. Les analyses pr√©c√©dentes seront prises en compte, mais vos instructions actuelles auront priorit√©.")
            
            # Process the data
            if st.button("Transformer les donn√©es", key="transform_csv"):
                st.header("R√©sultats de la transformation")
                
                # Initialize progress
                progress_bar = st.progress(0)
                status_text = st.empty()
                status_text.text("Initialisation du transformateur...")
                
                # Create a temporary file for the transformer
                with tempfile.NamedTemporaryFile(suffix='.csv', delete=False) as tmp_input:
                    df.to_csv(tmp_input.name, index=False)
                    tmp_input_path = tmp_input.name
                
                try:
                    # Initialize transformer
                    status_text.text("Initialisation du transformateur Mistral...")
                    progress_bar.progress(10)
                    
                    # Prepare context from history if enabled
                    analysis_context = ""
                    
                    if st.session_state.get("use_history", True):
                        status_text.text("R√©cup√©ration de l'historique d'analyse...")
                        progress_bar.progress(15)
                        
                        # Get historical context
                        analysis_context = get_combined_context(
                            dataset_context=similar_dataset, 
                            pdf_context=None,  # Nous pourrions ici s√©lectionner des PDFs pertinents si besoin
                            max_analyses=st.session_state.get("max_history", 3)
                        )
                        
                        # Afficher le contexte pour v√©rification (debug)
                        with st.expander("Contexte d'analyse combin√© (debug)", expanded=False):
                            st.code(analysis_context)

                    # Build the full context with clear priority for user instructions
                    user_context = st.session_state.get("user_context", "")
                    if user_context and analysis_context:
                        # Emphasize user instructions based on history_weight
                        if st.session_state.get("history_weight", 5) <= 3:  # Low weight to history
                            full_context = f"""INSTRUCTIONS UTILISATEUR (PRIORIT√â √âLEV√âE):
{user_context}

{analysis_context}"""
                        elif st.session_state.get("history_weight", 5) >= 7:  # High weight to history
                            full_context = f"""{user_context}

{analysis_context}"""
                        else:  # Balanced approach
                            full_context = f"""INSTRUCTIONS UTILISATEUR:
{user_context}

{analysis_context}"""
                    elif user_context:
                        full_context = user_context
                    else:
                        full_context = analysis_context
                    
                    if analysis_context:
                        st.info("‚ÑπÔ∏è Analyses pr√©c√©dentes int√©gr√©es au contexte. Vos instructions actuelles restent prioritaires.")
                        
                    transformer = DataTransformer(model_name=st.session_state.get("model_name", "mistral:latest"), 
                                                context_size=st.session_state.get("context_size", 4096))
                    
                    # Transform data
                    status_text.text("Application des transformations...")
                    progress_bar.progress(30)
                    
                    transformations = None if st.session_state.get("auto_detect", True) else st.session_state.get("transformations", [])
                    df_transformed, metadata = transformer.transform(df, transformations, full_context)
                    
                    # Display results
                    status_text.text("Traitement des r√©sultats...")
                    progress_bar.progress(70)
                    
                    # Display transformed data
                    st.subheader("Jeu de donn√©es transform√©")
                    st.dataframe(df_transformed.head())
                    
                    # Display transformation details
                    st.subheader("Transformations appliqu√©es")
                    
                    # Create columns for before/after stats
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Dimensions d'origine", f"{metadata['original_shape'][0]} √ó {metadata['original_shape'][1]}")
                    with col2:
                        st.metric("Dimensions transform√©es", f"{metadata['transformed_shape'][0]} √ó {metadata['transformed_shape'][1]}")
                    with col3:
                        missing_before = metadata['missing_values'].get('before', 0)
                        missing_after = metadata['missing_values'].get('after', 0)
                        reduction = missing_before - missing_after
                        reduction_pct = (1 - missing_after/max(1, missing_before))*100 if missing_before > 0 else 0
                        st.metric("R√©duction des valeurs manquantes", f"{reduction} ({reduction_pct:.1f}%)")
                    
                    # List all applied transformations
                    if metadata.get('transformations'):
                        for i, t in enumerate(metadata['transformations'], 1):
                            st.write(f"{i}. {t['details']}")
                    else:
                        st.info("Aucune transformation n'a √©t√© appliqu√©e.")
                    
                    # Show new columns
                    if metadata.get('new_columns'):
                        st.subheader("Nouvelles colonnes cr√©√©es")
                        st.write(", ".join(metadata['new_columns']))
                    
                    # Show removed columns
                    if metadata.get('removed_columns'):
                        st.subheader("Colonnes supprim√©es")
                        st.write(", ".join(metadata['removed_columns']))
                    
                    # Display Mistral's analysis if available
                    analysis_text = ""
                    if "analysis" in metadata and metadata["analysis"]:
                        st.subheader("Analyse IA")
                        st.info(metadata["analysis"])
                        analysis_text = metadata["analysis"]
                    
                    # Create download link for transformed data
                    st.markdown(create_download_link(df_transformed, "donnees_transformees.csv"), unsafe_allow_html=True)
                    
                    # Plot correlations for transformed data
                    correlation_plot = plot_correlations(df_transformed)
                    if correlation_plot:
                        st.subheader("Corr√©lations des variables (apr√®s transformation)")
                        st.pyplot(correlation_plot)
                    
                    # Save analysis to history
                    if st.session_state.get("use_history", True) and analysis_text:
                        dataset_info = get_dataset_info(df)
                        
                        # Create a descriptive name
                        dataset_description = f"Dataset avec {df.shape[0]} lignes et {df.shape[1]} colonnes"
                        if uploaded_file.name:
                            dataset_description += f" (Fichier: {uploaded_file.name})"
                        
                        # Add analysis to history
                        analysis_id = history.add_analysis(
                            dataset_name=dataset_name,
                            dataset_description=dataset_description,
                            analysis_text=analysis_text,
                            metadata=dataset_info
                        )
                        
                        if analysis_id:
                            st.success(f"‚úì Cette analyse a √©t√© sauvegard√©e et sera utilis√©e pour am√©liorer les analyses futures (ID: {analysis_id})")
                        else:
                            st.warning("‚ö†Ô∏è L'analyse n'a pas pu √™tre sauvegard√©e dans l'historique.")
                    
                    # Complete the progress
                    progress_bar.progress(100)
                    status_text.text("Transformation termin√©e avec succ√®s !")
                    
                except Exception as e:
                    st.error(f"Erreur pendant la transformation: {e}")
                    logger.error(f"Erreur de transformation: {e}")
                
                finally:
                    # Clean up the temporary file
                    if 'tmp_input_path' in locals() and os.path.exists(tmp_input_path):
                        os.unlink(tmp_input_path)
        
        except Exception as e:
            st.error(f"Erreur lors du chargement du fichier: {e}")
            logger.error(f"Erreur de chargement du fichier: {e}")

def process_pdf_data():
    """Fonction pour traiter les fichiers PDF"""
    # Initialiser l'historique des analyses PDF
    pdf_history = get_pdf_history()
    
    st.header("üìÑ Analyse de documents PDF")
    uploaded_pdf = st.file_uploader("Choisir un fichier PDF", type="pdf", key="pdf_uploader")
    
    if uploaded_pdf is not None:
        # Afficher les informations basiques sur le PDF
        pdf_size = len(uploaded_pdf.getvalue()) / 1024  # taille en KB
        st.success(f"PDF charg√© avec succ√®s: {uploaded_pdf.name} ({pdf_size:.1f} KB)")
        
        # Cr√©er un processeur PDF
        processor = PDFProcessor(model_name=st.session_state.get("model_name", "mistral:latest"),
                              context_size=st.session_state.get("context_size", 4096))
        
        # Extraire un aper√ßu des m√©tadonn√©es
        with st.expander("Aper√ßu du document", expanded=True):
            # Utiliser PyMuPDF pour r√©cup√©rer les m√©tadonn√©es de base
            pdf_bytes = io.BytesIO(uploaded_pdf.getvalue())
            
            try:
                import fitz  # PyMuPDF
                doc = fitz.open(stream=pdf_bytes.getvalue(), filetype="pdf")
                
                # Afficher les m√©tadonn√©es
                col1, col2 = st.columns(2)
                with col1:
                    st.write("Titre:", doc.metadata.get("title", "Non sp√©cifi√©"))
                    st.write("Auteur:", doc.metadata.get("author", "Non sp√©cifi√©"))
                    st.write("Nombre de pages:", len(doc))
                
                with col2:
                    st.write("Sujet:", doc.metadata.get("subject", "Non sp√©cifi√©"))
                    st.write("Cr√©ateur:", doc.metadata.get("creator", "Non sp√©cifi√©"))
                    st.write("Date de cr√©ation:", doc.metadata.get("creationDate", "Non sp√©cifi√©e"))
                
                # Afficher la premi√®re page comme aper√ßu
                if len(doc) > 0:
                    st.subheader("Aper√ßu du texte (premi√®re page)")
                    first_page = doc[0]
                    text_preview = first_page.get_text()[:500] + "..." if len(first_page.get_text()) > 500 else first_page.get_text()
                    st.text_area("Texte extrait", text_preview, height=200)
                
                doc.close()
            except Exception as e:
                st.warning(f"Impossible d'afficher l'aper√ßu du PDF: {e}")
        
        # Essayer de trouver un PDF similaire dans l'historique
        similar_pdf = None
        if st.session_state.get("use_history", True):
            try:
                import fitz
                doc = fitz.open(stream=pdf_bytes.getvalue(), filetype="pdf")
                metadata = {
                    "title": doc.metadata.get("title", ""),
                    "author": doc.metadata.get("author", ""),
                    "page_count": len(doc)
                }
                doc.close()
                
                similar_pdf = pdf_history.find_similar_pdf(metadata)
                if similar_pdf and similar_pdf in pdf_history.index["documents"]:
                    pdf_name = pdf_history.index["documents"][similar_pdf]["name"]
                    st.info(f"üìå Ce document PDF semble similaire √† '{pdf_name}' que vous avez d√©j√† analys√©. Les analyses pr√©c√©dentes seront prises en compte.")
            except Exception as e:
                logger.error(f"Erreur lors de la recherche de PDF similaires: {e}")
        
        # Contexte utilisateur pour l'analyse
        user_context_pdf = st.text_area(
            "Instructions pour l'analyse du PDF (optionnel)",
            help="Fournissez des instructions sp√©cifiques pour guider l'analyse de ce document",
            key="user_context_pdf"
        )
        
        # Bouton d'analyse du PDF
        if st.button("Analyser le PDF", key="analyze_pdf"):
            st.header("R√©sultats de l'analyse")
            
            # Initialize progress
            progress_bar = st.progress(0)
            status_text = st.empty()
            status_text.text("Initialisation de l'analyse du PDF...")
            
            try:
                # Pr√©parer le contexte √† partir de l'historique si activ√©
                analysis_context = ""
                
                if st.session_state.get("use_history", True):
                    status_text.text("R√©cup√©ration de l'historique d'analyse PDF...")
                    progress_bar.progress(10)
                    
                    # R√©cup√©rer le contexte des analyses pr√©c√©dentes
                    analysis_context = get_combined_context(
                        dataset_context=None,  # Nous pourrions ici s√©lectionner des datasets pertinents
                        pdf_context=similar_pdf,
                        max_analyses=st.session_state.get("max_history", 3)
                    )
                    # Afficher le contexte pour v√©rification
                    with st.expander("Contexte d'analyse combin√© (debug)", expanded=False):
                        st.code(analysis_context)
                
                # Construire le contexte complet
                if user_context_pdf and analysis_context:
                    # Mettre l'accent sur les instructions utilisateur
                    full_context = f"""INSTRUCTIONS UTILISATEUR (PRIORIT√â √âLEV√âE):
{user_context_pdf}

{analysis_context}"""
                elif user_context_pdf:
                    full_context = user_context_pdf
                else:
                    full_context = analysis_context
                
                # Informer l'utilisateur sur l'utilisation du contexte
                if analysis_context:
                    st.info("‚ÑπÔ∏è Analyses pr√©c√©dentes int√©gr√©es au contexte. Vos instructions actuelles restent prioritaires.")
                
                # Traiter le PDF
                status_text.text("Extraction et analyse du texte du PDF...")
                progress_bar.progress(30)
                
                # Analyser le PDF (r√©initialiser BytesIO pour √©viter les erreurs EOF)
                pdf_bytes = io.BytesIO(uploaded_pdf.getvalue())
                pdf_result = processor.process_pdf(pdf_bytes, context=full_context)
                
                if pdf_result["success"]:
                    # Extraction r√©ussie
                    progress_bar.progress(70)
                    status_text.text("Analyse du PDF termin√©e, pr√©paration des r√©sultats...")
                    
                    # Afficher les m√©tadonn√©es extraites
                    st.subheader("M√©tadonn√©es du document")
                    metadata = pdf_result["metadata"]
                    
                    # Cr√©er une pr√©sentation en colonnes des m√©tadonn√©es
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Pages", metadata.get("page_count", "N/A"))
                        st.write("Titre:", metadata.get("title", "Non sp√©cifi√©"))
                    with col2:
                        st.metric("Mots", metadata.get("word_count", "N/A"))
                        st.write("Auteur:", metadata.get("author", "Non sp√©cifi√©"))
                    with col3:
                        st.metric("Caract√®res", metadata.get("character_count", "N/A"))
                        st.write("Sujet:", metadata.get("subject", "Non sp√©cifi√©"))
                    
                    # Afficher les r√©sultats de l'analyse
                    st.subheader("Analyse du contenu")
                    analysis = pdf_result["analysis"]
                    
                    # R√©sum√© principal
                    if analysis.get("summary"):
                        st.markdown("#### R√©sum√© principal")
                        st.info(analysis["summary"])
                    
                    # Th√®mes cl√©s
                    if analysis.get("key_themes"):
                        st.markdown("#### Th√®mes cl√©s")
                        for theme in analysis["key_themes"]:
                            st.markdown(f"‚Ä¢ {theme}")
                    
                    # Insights
                    if analysis.get("insights"):
                        st.markdown("#### Insights")
                        for insight in analysis["insights"]:
                            st.markdown(f"‚Ä¢ {insight}")
                    
                    # Tableau extrait (si disponible)
                    if pdf_result.get("tables") and len(pdf_result["tables"]) > 0:
                        st.subheader(f"Tableaux extraits ({len(pdf_result['tables'])})")
                        for i, table_dict in enumerate(pdf_result["tables"], 1):
                            try:
                                # Convertir le dictionnaire en DataFrame
                                table_df = pd.DataFrame.from_dict(table_dict)
                                st.markdown(f"**Tableau {i}:**")
                                st.dataframe(table_df)
                            except Exception as e:
                                st.warning(f"Impossible d'afficher le tableau {i}: {e}")
                    
                    # Cr√©er un lien de t√©l√©chargement pour le r√©sum√© d'analyse
                    if analysis.get("full_analysis"):
                        full_analysis = analysis["full_analysis"]
                        formatted_analysis = f"""# Analyse du document: {uploaded_pdf.name}
Date: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M")}

## R√©sum√©
{analysis.get("summary", "R√©sum√© non disponible")}

## Th√®mes cl√©s
{chr(10).join("- " + theme for theme in analysis.get("key_themes", ["Th√®mes non disponibles"]))}

## Insights
{chr(10).join("- " + insight for insight in analysis.get("insights", ["Insights non disponibles"]))}

## Analyse compl√®te
{full_analysis}
"""
                        st.markdown(
                            create_text_download_link(formatted_analysis, f"analyse_{uploaded_pdf.name.replace('.pdf', '.txt')}"),
                            unsafe_allow_html=True
                        )
                    
                    # Sauvegarder l'analyse dans l'historique PDF
                    if st.session_state.get("use_history", True):
                        pdf_id = pdf_result["pdf_id"]
                        analysis_id = pdf_history.add_pdf_analysis(
                            pdf_id=pdf_id,
                            pdf_name=uploaded_pdf.name,
                            analysis_result=analysis,
                            metadata=metadata
                        )
                        
                        if analysis_id:
                            st.success(f"‚úì Cette analyse PDF a √©t√© sauvegard√©e et sera utilis√©e pour am√©liorer les analyses futures (ID: {analysis_id})")
                        else:
                            st.warning("‚ö†Ô∏è L'analyse PDF n'a pas pu √™tre sauvegard√©e dans l'historique.")
                
                else:
                    # √âchec de l'extraction
                    st.error(f"√âchec de l'analyse du PDF: {pdf_result.get('error', 'Erreur inconnue')}")
                
                # Terminer la progression
                progress_bar.progress(100)
                status_text.text("Analyse du PDF termin√©e avec succ√®s !")
                
            except Exception as e:
                st.error(f"Erreur lors de l'analyse du PDF: {e}")
                logger.error(f"Erreur d'analyse PDF: {e}")
    
    else:
        st.info("Veuillez charger un fichier PDF pour commencer l'analyse.")

def process_visualizations():
    """Fonction pour g√©rer les visualisations interactives"""
    
    st.header("üìä Visualisations interactives")
    
    # V√©rifier si un DataFrame est d√©j√† charg√© dans la session
    if "df" in st.session_state:
        # Afficher un aper√ßu du DataFrame charg√©
        st.write(f"**Donn√©es charg√©es**: {st.session_state.get('df_name', 'Donn√©es sans nom')} - {st.session_state['df'].shape[0]} lignes, {st.session_state['df'].shape[1]} colonnes")
        
        # Appeler la fonction de visualisation
        create_charts()
    else:
        # Permettre √† l'utilisateur de charger un fichier directement depuis cet onglet
        st.info("Aucune donn√©e charg√©e. Vous pouvez charger un fichier CSV ci-dessous.")
        uploaded_file = st.file_uploader("Choisir un fichier CSV pour les visualisations", type="csv", key="viz_uploader")
        
        if uploaded_file is not None:
            try:
                df = pd.read_csv(uploaded_file)
                st.session_state["df"] = df
                st.session_state["df_name"] = uploaded_file.name
                st.success(f"Donn√©es charg√©es avec succ√®s : {df.shape[0]} lignes, {df.shape[1]} colonnes")
                
                # Afficher un aper√ßu
                st.dataframe(df.head())
                
                # Appeler la fonction de visualisation
                create_charts()
            except Exception as e:
                st.error(f"Erreur lors du chargement du fichier: {e}")

def show_history_tab():
    """Affiche l'historique des analyses CSV et PDF"""
    
    st.header("üìö Historique des analyses")
    
    # R√©cup√©rer les historiques
    history = get_analysis_history()
    pdf_history = get_pdf_history()
    
    # Cr√©er des sous-onglets pour diff√©rencier CSV et PDF
    tabs = st.tabs(["CSV", "PDF"])
    
    # Onglet CSV
    with tabs[0]:
        recent_analyses = history.get_recent_analyses(limit=10)
        
        if recent_analyses:
            st.write(f"Vous avez {len(history.index['analyses'])} analyses CSV sauvegard√©es sur {len(history.index['datasets'])} jeux de donn√©es diff√©rents.")
            
            # Afficher les analyses r√©centes
            st.subheader("Analyses CSV r√©centes")
            for analysis in recent_analyses:
                with st.expander(f"{analysis['dataset_name']} - {analysis['timestamp'][:16].replace('T', ' ')}"):
                    st.write(f"**Description**: {analysis.get('dataset_description', 'Non disponible')}")
                    if "metadata" in analysis and analysis["metadata"]:
                        meta = analysis["metadata"]
                        st.write(f"**Dimensions**: {meta.get('dimensions', 'Non disponible')}")
                    st.write("**Analyse**:")
                    st.info(analysis.get("analysis", "Contenu non disponible"))
            
            # Afficher les datasets
            st.subheader("Datasets analys√©s")
            for dataset_name, dataset_info in history.index["datasets"].items():
                with st.expander(f"{dataset_name} ({dataset_info['analyses_count']} analyses)"):
                    st.write(f"**Premi√®re analyse**: {dataset_info['first_analysis'][:16].replace('T', ' ')}")
                    st.write(f"**Derni√®re analyse**: {dataset_info['last_analysis'][:16].replace('T', ' ')}")
                    
                    # Afficher la derni√®re analyse pour ce dataset
                    last_analysis_id = dataset_info['analyses'][-1]
                    last_analysis = history.get_analysis(last_analysis_id)
                    if last_analysis:
                        st.write("**Derni√®re analyse**:")
                        st.info(last_analysis.get("analysis", "Aucune analyse disponible"))

                    else:
                        st.info("Aucune analyse CSV dans l'historique")

    # Onglet PDF
    with tabs[1]:
        pdf_analyses = pdf_history.get_recent_pdf_analyses(limit=10)
        
        if pdf_analyses:
            st.write(f"Vous avez {len(pdf_history.index['analyses'])} analyses PDF sauvegard√©es sur {len(pdf_history.index['documents'])} documents diff√©rents.")
            
            # Afficher les analyses r√©centes
            st.subheader("Analyses PDF r√©centes")
            for analysis in pdf_analyses:
                with st.expander(f"{analysis['pdf_name']} - {analysis['timestamp'][:16].replace('T', ' ')}"):
                    st.write(f"**ID du PDF**: {analysis.get('pdf_id', 'Non disponible')}")
                    
                    # Afficher les m√©tadonn√©es
                    if "metadata" in analysis and analysis["metadata"]:
                        meta = analysis["metadata"]
                        st.write(f"**Pages**: {meta.get('page_count', 'N/A')}, **Mots**: {meta.get('word_count', 'N/A')}")
                    
                    # Afficher le r√©sum√©
                    if "analysis" in analysis and "summary" in analysis["analysis"]:
                        st.write("**R√©sum√©**:")
                        st.info(analysis["analysis"]["summary"])
                    
                    # Afficher les th√®mes
                    if "analysis" in analysis and "key_themes" in analysis["analysis"]:
                        st.write("**Th√®mes cl√©s**:")
                        for theme in analysis["analysis"]["key_themes"]:
                            st.markdown(f"‚Ä¢ {theme}")
        else:
            st.info("Aucune analyse PDF dans l'historique")

def show_settings():
    """Affiche les param√®tres de l'application"""
    st.header("‚öôÔ∏è Param√®tres")
    
    # Param√®tres du mod√®le
    st.subheader("Param√®tres du mod√®le")
    
    # Initialiser les param√®tres par d√©faut s'ils n'existent pas
    if "model_name" not in st.session_state:
        st.session_state["model_name"] = "mistral:latest"
    if "context_size" not in st.session_state:
        st.session_state["context_size"] = 4096
    
    # Mod√®le √† utiliser
    st.session_state["model_name"] = st.selectbox(
        "Mod√®le √† utiliser",
        ["mistral:latest", "llama3:latest", "llama2:13b", "mixtral:latest"],
        index=["mistral:latest", "llama3:latest", "llama2:13b", "mixtral:latest"].index(st.session_state["model_name"])
    )
    
    # Taille du contexte
    st.session_state["context_size"] = st.slider(
        "Taille du contexte (tokens)",
        min_value=1024,
        max_value=16384,
        value=st.session_state["context_size"],
        step=1024
    )
    
    # Param√®tres de l'historique
    st.subheader("Param√®tres de l'historique")
    
    # Activation de l'historique
    st.session_state["use_history"] = st.toggle(
        "Utiliser l'historique des analyses",
        value=st.session_state.get("use_history", True),
        help="Si activ√©, les analyses pr√©c√©dentes sont utilis√©es pour am√©liorer les analyses futures"
    )
    
    # Nombre maximal d'analyses √† utiliser
    st.session_state["max_history"] = st.slider(
        "Nombre maximum d'analyses historiques √† utiliser",
        min_value=1,
        max_value=10,
        value=st.session_state.get("max_history", 3),
        step=1,
        help="Nombre maximum d'analyses pr√©c√©dentes √† inclure dans le contexte"
    )
    
    # Poids de l'historique
    st.session_state["history_weight"] = st.slider(
        "Poids de l'historique (1-10)",
        min_value=1,
        max_value=10,
        value=st.session_state.get("history_weight", 5),
        step=1,
        help="D√©termine l'importance des analyses pass√©es par rapport aux instructions actuelles (1 = faible, 10 = √©lev√©)"
    )
    
    # Param√®tres de transformation
    st.subheader("Param√®tres de transformation")
    
    # Auto-d√©tection des transformations
    st.session_state["auto_detect"] = st.toggle(
        "Auto-d√©tection des transformations",
        value=st.session_state.get("auto_detect", True),
        help="Si activ√©, l'IA d√©tecte automatiquement les transformations √† appliquer"
    )
    
    # Transformations manuelles (visible si auto-d√©tection d√©sactiv√©e)
    if not st.session_state["auto_detect"]:
        # Liste des transformations possibles
        all_transformations = [
            "Traitement des valeurs manquantes",
            "Normalisation des donn√©es",
            "D√©tection et traitement des outliers",
            "Encodage des variables cat√©gorielles",
            "Cr√©ation de nouvelles variables",
            "R√©duction de dimensionnalit√©",
            "Filtrage des donn√©es",
            "Agr√©gation des donn√©es"
        ]
        
        # Initialiser les transformations si non d√©finies
        if "transformations" not in st.session_state:
            st.session_state["transformations"] = ["Traitement des valeurs manquantes"]
        
        # S√©lection multiple des transformations
        st.session_state["transformations"] = st.multiselect(
            "Transformations √† appliquer",
            all_transformations,
            default=st.session_state["transformations"]
        )

# Page principale
def main():
    """Application principale"""
    
    # Titre et description
    st.title("ORBIT - Data Transformation")
    st.image("C:\\Users\\Francis\\Desktop\\Data_cleaner\\PDF_analysis\\orbit-icon-13.jpg")
    st.write("Une application pour transformer vos donn√©es CSV et analyser vos PDF avec IA")
    
    # Onglets pour les diff√©rentes sections
# Onglets pour les diff√©rentes sections
tabs = st.tabs(["CSV", "PDF", "Visualisations", "Historique", "Param√®tres"])

# Onglet CSV
with tabs[0]:
    process_csv_data()

# Onglet PDF
with tabs[1]:
    process_pdf_data()

# Onglet Visualisations
with tabs[2]:
    process_visualizations()  # Nouvelle fonction √† cr√©er

# Onglet Historique
with tabs[3]:
    show_history_tab()

# Onglet Param√®tres
with tabs[4]:
    show_settings()
    

    # Contexte utilisateur pour l'ensemble de l'application
    with st.sidebar:
        st.header("Instructions utilisateur")
        
        st.session_state["user_context"] = st.text_area(
            "Instructions pour l'analyse",
            value=st.session_state.get("user_context", ""),
            height=300,
            help="D√©crivez votre objectif d'analyse ou posez des questions sur vos donn√©es. Par exemple: 'Identifiez les tendances principales', 'Analysez les relations entre variables', ou 'Que pouvez-vous me dire sur ces donn√©es?'",
            key="user_context_area"
        )
        
        # Ajouter un exemple de prompt utile
        with st.expander("üìù Exemples de prompts efficaces"):
            st.markdown("""
            **Prompts g√©n√©riques utiles:**
            - "Analysez ce jeu de donn√©es et identifiez les insights principaux."
            - "Pouvez-vous me dire quelles variables semblent avoir le plus d'impact sur [variable cible]?"
            - "Quelles tendances ou patterns voyez-vous dans ces donn√©es?"
            - "Comment pourrais-je am√©liorer la qualit√© de ce jeu de donn√©es?"
            
            **Pour les CSV:**
            - "Y a-t-il des corr√©lations significatives entre les variables?"
            - "Comment pourrais-je segmenter ces donn√©es pour mieux comprendre [ph√©nom√®ne]?"
            
            **Pour les PDF:**
            - "R√©sumez les points cl√©s de ce document et identifiez les th√®mes principaux."
            - "Quels sont les arguments principaux pr√©sent√©s dans ce document?"
            """)
        
        # Afficher des informations sur l'historique
        if st.session_state.get("use_history", True):
            history = get_analysis_history()
            pdf_history = get_pdf_history()
            
            st.info(f"""
            **√âtat de l'historique**:
            - {len(history.index['analyses'])} analyses CSV
            - {len(pdf_history.index['analyses'])} analyses PDF
            
            Historique activ√© (poids: {st.session_state.get("history_weight", 5)}/10)
            """)
        else:
            st.warning("L'historique est actuellement d√©sactiv√©")
        
        # Afficher les cr√©dits
        st.markdown("---")
        st.markdown("¬© 2024 - Application de transformation de donn√©es avec m√©moire")

if __name__ == "__main__":
    main()