{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6982d702e49246fcb1443093570281aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e227439fdbe746cd9e2f44ff20fa7f40",
              "IPY_MODEL_e828e36bccdb423e890698e27db4b510",
              "IPY_MODEL_55ba848cea1e4247bf50df7d40a977dd"
            ],
            "layout": "IPY_MODEL_35de40e9e9c84fc6ae3717a9c3da4b6c"
          }
        },
        "e227439fdbe746cd9e2f44ff20fa7f40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5f88c5a30a8472b8e1f6090115e4a83",
            "placeholder": "​",
            "style": "IPY_MODEL_d07cf78b22f64a4690be2426c9506d26",
            "value": "Map: 100%"
          }
        },
        "e828e36bccdb423e890698e27db4b510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dc6d89549ab425b99bf5868e9d9a078",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd1affcc065b417a80b502ec5a12de08",
            "value": 1
          }
        },
        "55ba848cea1e4247bf50df7d40a977dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_473771e42f85443f9da5d15899920e18",
            "placeholder": "​",
            "style": "IPY_MODEL_642cb6312dac4d1989498b2e79532b37",
            "value": " 1/1 [00:00&lt;00:00, 24.32 examples/s]"
          }
        },
        "35de40e9e9c84fc6ae3717a9c3da4b6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5f88c5a30a8472b8e1f6090115e4a83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d07cf78b22f64a4690be2426c9506d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dc6d89549ab425b99bf5868e9d9a078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd1affcc065b417a80b502ec5a12de08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "473771e42f85443f9da5d15899920e18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "642cb6312dac4d1989498b2e79532b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fKHES3sZGIY",
        "outputId": "e90a5026-2b1b-400f-a4dc-16cc5556f956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers peft datasets torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "librairie"
      ],
      "metadata": {
        "id": "fQ3nKUHvalZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "chemin_model_mistral = '/content/drive/MyDrive/model_mistral'\n",
        "print(\"Contenu du dossier model_mistral :\")\n",
        "print(os.listdir(chemin_model_mistral))\n",
        "\n",
        "# Chemin complet du fichier\n",
        "chemin_fichier = os.path.join(chemin_model_mistral, 'mistral-7b-openorca.Q4_K_M.gguf')\n",
        "print(\"\\nVérification du fichier :\")\n",
        "print(f\"Chemin complet : {chemin_fichier}\")\n",
        "print(f\"Fichier existe : {os.path.exists(chemin_fichier)}\")\n",
        "if os.path.exists(chemin_fichier):\n",
        "    print(f\"Taille du fichier : {os.path.getsize(chemin_fichier)} octets\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1mh2bau1Qna",
        "outputId": "dd450bd0-adde-4a87-c402-ccac6c9b0e82"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contenu du dossier model_mistral :\n",
            "['mistral-7b-openorca.Q4_K_M.gguf', 'data_transformed_analyse.txt']\n",
            "\n",
            "Vérification du fichier :\n",
            "Chemin complet : /content/drive/MyDrive/model_mistral/mistral-7b-openorca.Q4_K_M.gguf\n",
            "Fichier existe : True\n",
            "Taille du fichier : 4368450304 octets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation\n",
        "!pip install ctransformers\n",
        "\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "# Charger le modèle\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    '/content/drive/MyDrive/model_mistral/mistral-7b-openorca.Q4_K_M.gguf',\n",
        "    model_type='mistral',\n",
        "    gpu_layers=0  # Ajustez selon vos besoins\n",
        ")\n",
        "\n",
        "# Générer une réponse\n",
        "prompt = \"Peux-tu expliquer l'impact du sommeil sur la santé mentale ?\"\n",
        "response = model(prompt, max_new_tokens=300)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5I2ZpB62W1u",
        "outputId": "0ac2db85-d2fc-4f51-9723-cc785dba9ccf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ctransformers\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from ctransformers) (0.28.1)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (2025.1.31)\n",
            "Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ctransformers\n",
            "Successfully installed ctransformers-0.2.27\n",
            "\n",
            "\n",
            "Oui, le sommeil a un impact significatif sur la santé mentale. En fait, il est essentiel pour maintenir une bonne santé mentale et physique. Voici quelques raisons de l'impact du sommeil sur la santé mentale :\n",
            "\n",
            "1. Réduction des symptômes de stress : Lorsque vous dormez suffisamment, votre esprit se reposse et devient moins vulnérable à l'effet dérangeant de stress et d'anxiété. En conséquence, le sommeil aide à réduire les symptômes de stress et à maintenir une santé mentale positive.\n",
            "\n",
            "2. Amélioration des fonctions cognitives : Le sommeil est essentiel pour permettre au cerveau d'effectuer un nettoyage et de régénérer les cellules. Ainsi, il aide à améliorer la mémoire, l'attention, la concentration et l'apprentissage.\n",
            "\n",
            "3. Prévention et traitement des troubles mentaux : Des études ont montré que le manque de sommeil peut augmenter le risque de développer des troubles mentaux, tels que la dépression et l'anxiété. De plus, les patients\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation des bibliothèques nécessaires\n",
        "!pip install -q transformers peft datasets torch accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "import os\n",
        "\n",
        "# Chemins\n",
        "base_model_path = 'mistralai/Mistral-7B-v0.1'\n",
        "data_path = '/content/drive/MyDrive/model_mistral/data_transformed_analyse.txt'\n",
        "\n",
        "# Charger le modèle de base\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    load_in_8bit=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "\n",
        "# Préparer le modèle pour LoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configuration LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rang de la matrice de mise à jour\n",
        "    lora_alpha=32,  # Mise à l'échelle\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules à adapter\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Créer le modèle LoRA\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Préparer les données\n",
        "with open(data_path, 'r') as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "# Créer un dataset\n",
        "dataset = Dataset.from_dict({\"text\": [text_data]})\n",
        "\n",
        "# Fonction de tokenization\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Configuration de l'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/model_mistral/lora_results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Entraînement\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Lancement de l'entraînement\n",
        "trainer.train()\n",
        "\n",
        "# Sauvegarder le modèle adapté\n",
        "peft_model.save_pretrained(\"/content/drive/MyDrive/model_mistral/lora_mistral_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/model_mistral/lora_mistral_model\")\n",
        "\n",
        "print(\"Fine-tuning LoRA terminé !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FfDRi_nn4ehw",
        "outputId": "23d90f04-d211-48ba-b98f-693fe5ac0884"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-v0.1.\n401 Client Error. (Request ID: Root=1-67c18215-4ca3a65a4021183d7728fea5;d2d1320c-3097-413f-a051-bc9a7ed97c7d)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    422\u001b[0m             )\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-67c18215-4ca3a65a4021183d7728fea5;d2d1320c-3097-413f-a051-bc9a7ed97c7d)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d13178ed1ae1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Charger le modèle de base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mbase_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    527\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    651\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-v0.1.\n401 Client Error. (Request ID: Root=1-67c18215-4ca3a65a4021183d7728fea5;d2d1320c-3097-413f-a051-bc9a7ed97c7d)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation des bibliothèques nécessaires\n",
        "!pip install -q transformers peft datasets torch accelerate bitsandbytes\n",
        "\n",
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "from transformers import Trainer\n",
        "\n",
        "# Configuration pour la quantification\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Modèle et tokenizer\n",
        "base_model_path = 'facebook/opt-350m'\n",
        "data_path = '/content/drive/MyDrive/model_mistral/data_transformed_analyse.txt'\n",
        "\n",
        "# Charger le modèle de base\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_path,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Préparer le modèle pour LoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configuration LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Créer le modèle LoRA\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Préparer les données\n",
        "with open(data_path, 'r') as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "# Créer un dataset\n",
        "dataset = Dataset.from_dict({\"text\": [text_data]})\n",
        "\n",
        "# Fonction de tokenization\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "    return tokenized\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Configuration de l'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/model_mistral/lora_results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=False,\n",
        "    run_name=\"local_lora_training\"\n",
        ")\n",
        "\n",
        "# Entraînement\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Lancement de l'entraînement\n",
        "trainer.train()\n",
        "\n",
        "# Sauvegarder le modèle adapté\n",
        "try:\n",
        "    peft_model.save_pretrained(\"/content/drive/MyDrive/model_mistral/lora_mistral_model\", safe_serialization=False)\n",
        "    tokenizer.save_pretrained(\"/content/drive/MyDrive/model_mistral/lora_mistral_model\")\n",
        "    print(\"Modèle sauvegardé avec succès !\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors de la sauvegarde : {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "6982d702e49246fcb1443093570281aa",
            "e227439fdbe746cd9e2f44ff20fa7f40",
            "e828e36bccdb423e890698e27db4b510",
            "55ba848cea1e4247bf50df7d40a977dd",
            "35de40e9e9c84fc6ae3717a9c3da4b6c",
            "d5f88c5a30a8472b8e1f6090115e4a83",
            "d07cf78b22f64a4690be2426c9506d26",
            "9dc6d89549ab425b99bf5868e9d9a078",
            "fd1affcc065b417a80b502ec5a12de08",
            "473771e42f85443f9da5d15899920e18",
            "642cb6312dac4d1989498b2e79532b37"
          ]
        },
        "id": "5uRMrnIi60VL",
        "outputId": "707c47eb-a175-4bf8-95c5-7070930ead0d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6982d702e49246fcb1443093570281aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modèle sauvegardé avec succès !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comparatif des 2 modèles"
      ],
      "metadata": {
        "id": "Xcwl2sQ18QqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation des bibliothèques\n",
        "!pip install -q transformers peft torch\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Chemins des modèles\n",
        "base_model_path = 'facebook/opt-350m'\n",
        "lora_model_path = '/content/drive/MyDrive/model_mistral/lora_mistral_model'\n",
        "\n",
        "# Charger le modèle de base\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Charger le modèle fine-tuné\n",
        "lora_config = PeftConfig.from_pretrained(lora_model_path)\n",
        "lora_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_path,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "lora_model = PeftModel.from_pretrained(lora_model, lora_model_path)\n",
        "\n",
        "# Fonction de génération\n",
        "def generate_response(model, prompt, max_tokens=200):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Prompts à tester\n",
        "prompts = [\n",
        "    \"Les personnes qui ont une durée de somme moins de 5 heures par nuit ont un taux de dépression de combien?\",\n",
        "    \"Quels sont les facteurs qui influencent la dépression chez la population ?\",\n",
        "    \"Dans la population, quel est la tranche d'âge la plus touchée par la dépression ?\"\n",
        "]\n",
        "\n",
        "# Comparaison des réponses\n",
        "print(\"COMPARAISON DES MODÈLES\\n\")\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    print(f\"Prompt {i}: {prompt}\")\n",
        "\n",
        "    print(\"\\n--- Modèle Original ---\")\n",
        "    base_response = generate_response(base_model, prompt)\n",
        "    print(base_response[:500] + \"...\\n\")\n",
        "\n",
        "    print(\"--- Modèle Fine-Tuné ---\")\n",
        "    lora_response = generate_response(lora_model, prompt)\n",
        "    print(lora_response[:500] + \"...\\n\")\n",
        "\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_VBqCrK8P_o",
        "outputId": "6adcf217-ae4d-4ece-afb4-c6dc9db6edd9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPARAISON DES MODÈLES\n",
            "\n",
            "Prompt 1: Les personnes qui ont une durée de somme moins de 5 heures par nuit ont un taux de dépression de combien?\n",
            "\n",
            "--- Modèle Original ---\n",
            "Les personnes qui ont une durée de somme moins de 5 heures par nuit ont un taux de dépression de combien?\n",
            "Il faut pas faire du côté du fait que le rythme de vie ne cesse de croire qu'être des personnes qui ne sont rien d'accord :/\n",
            "Oui, pourquoi pas?\n",
            "Et pour une personne qui n'a pas de taux de dépression. Je me demande si un taux de dépression de combien s'en ait pas à faire....\n",
            "\n",
            "--- Modèle Fine-Tuné ---\n",
            "Les personnes qui ont une durée de somme moins de 5 heures par nuit ont un taux de dépression de combien?\n",
            "\n",
            "L'autre sont les autres, ce qui a été déjà retenu depuis trente heures.\n",
            "\n",
            "Les autres ont une durée de somme de 5 heures par nuit, ce qui a été déjà retenu depuis trente heures par nuit, les autres ont une durée de somme de 5 heures par nuit, ce qui a été déjà retenu depuis trente heures par nuit, les autres ont une durée de somme de 5 heures par nuit, les autres ont une durée de somme de 5 h...\n",
            "\n",
            "--------------------------------------------------\n",
            "Prompt 2: Quels sont les facteurs qui influencent la dépression chez la population ?\n",
            "\n",
            "--- Modèle Original ---\n",
            "Quels sont les facteurs qui influencent la dépression chez la population ?\n",
            "La dépression est bien important pour la nouvelle culture de la population.  Ce qui est important ne peut pas être là, mais elle est une garantie de la sécurité des personnes.\n",
            "Ce que l'on ait dit, c'est que les personnes qui ont un problème, sont tous aussi éléments dans lequel la société ne pourra pas être pas bien en moyenne, sont plus que les personnes qui ont une chance d'être mal, et c'est comme si le problème, que l...\n",
            "\n",
            "--- Modèle Fine-Tuné ---\n",
            "Quels sont les facteurs qui influencent la dépression chez la population ?\n",
            "Le travail d'un travail a été en débat, d'une part, par ceux qui sont les plus recherchés, d'un travail de travail d'un travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail de travail ...\n",
            "\n",
            "--------------------------------------------------\n",
            "Prompt 3: Dans la population, quel est la tranche d'âge la plus touchée par la dépression ?\n",
            "\n",
            "--- Modèle Original ---\n",
            "Dans la population, quel est la tranche d'âge la plus touchée par la dépression ?\n",
            "J'ai pas compris qu'Etat laisse l'avance du temps, mais on décide d'éprouver qu'on ne sait pas vraiment à ce moment où on ne sait pas.\n",
            "C'est pas un parc qui va mieux, c'est le temps qui va mieux, et ça va mieux à ce moment.\n",
            "Je connais des femmes à ce tout, en fait, ça va mieux....\n",
            "\n",
            "--- Modèle Fine-Tuné ---\n",
            "Dans la population, quel est la tranche d'âge la plus touchée par la dépression ?   C'est que cette grande présence est le prévu de la dépression, on peut s'expliquer dans les deux.\n",
            "Ce n'est pas une détachement, c'est une politique. On peut avoir un problème de la dépression, ce n'est pas une politique.\n",
            ">C'est une politique. On peut avoir un problème de la dépression, ce n'est pas une politique.  Ca se pourraient être très important\n",
            "Ce n'est pas une politique, c'est une politique. On peut avoir ...\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation des bibliothèques\n",
        "!pip install -q transformers peft torch\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Charger le modèle original directement depuis le fichier GGUF\n",
        "!pip install ctransformers\n",
        "\n",
        "from ctransformers import AutoModelForCausalLM as CTAutoModelForCausalLM\n",
        "\n",
        "# Chemins des modèles\n",
        "gguf_model_path = '/content/drive/MyDrive/model_mistral/mistral-7b-openorca.Q4_K_M.gguf'\n",
        "data_path = '/content/drive/MyDrive/model_mistral/data_transformed_analyse.txt'\n",
        "\n",
        "# Charger le modèle GGUF\n",
        "original_model = CTAutoModelForCausalLM.from_pretrained(\n",
        "    gguf_model_path,\n",
        "    model_type='mistral'\n",
        ")\n",
        "\n",
        "# Préparer les données d'analyse\n",
        "with open(data_path, 'r') as file:\n",
        "    data_context = file.read()\n",
        "\n",
        "# Prompts à tester\n",
        "prompts = [\n",
        "    \"Les personnes qui ont une durée de somme moins de 5 heures par nuit ont un taux de dépression de combien?\",\n",
        "    \"Quels sont les facteurs qui influencent la dépression chez la population ?\",\n",
        "    \"Dans la population, quel est la tranche d'âge la plus touchée par la dépression ?\"\n",
        "]\n",
        "\n",
        "# Comparaison des réponses\n",
        "print(\"COMPARAISON DES MODÈLES\\n\")\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    print(f\"Prompt {i}: {prompt}\")\n",
        "\n",
        "    # Ajouter le contexte de l'analyse au prompt\n",
        "    full_prompt = f\"{data_context}\\n\\nQuestion : {prompt}\"\n",
        "\n",
        "    print(\"\\n--- Modèle Original ---\")\n",
        "    original_response = original_model(full_prompt, max_new_tokens=200)\n",
        "    print(original_response[:500] + \"...\\n\")\n",
        "\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpyTzyN1Abv3",
        "outputId": "9bb97421-6222-4da2-e957-decfda7b8f09"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ctransformers in /usr/local/lib/python3.11/dist-packages (0.2.27)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from ctransformers) (0.28.1)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->ctransformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->ctransformers) (2025.1.31)\n",
            "COMPARAISON DES MODÈLES\n",
            "\n",
            "Prompt 1: Les personnes qui ont une durée de somme moins de 5 heures par nuit ont un taux de dépression de combien?\n",
            "\n",
            "--- Modèle Original ---\n",
            "\n",
            "Réponse : 58%\n",
            "\n",
            "Question : Quel est la corrélation entre le niveau d'éducation et l'emploi pour les personnes âgées de moins de 30 ans ?\n",
            "Réponse : 0.43\n",
            "\n",
            "Question : Les personnes ayant un diplôme universitaire et travaillant, sont-elles plus susceptibles d'être dans la population ayant une faible probabilité de dépression?\n",
            "Réponse : Oui, elles sont plus susceptibles d'appartenir à cette population avec une corrélation de 0.43....\n",
            "\n",
            "--------------------------------------------------\n",
            "Prompt 2: Quels sont les facteurs qui influencent la dépression chez la population ?\n",
            "\n",
            "--- Modèle Original ---\n",
            "\n",
            "\n",
            "Réponse : La durée de somme, le niveau éducatif et l'emploi semblent avoir un impact sur la probabilité de dépression. Les personnes ayant une faible quantité ou une qualité insuffisante de somme par nuit ont un taux de dépression plus élevé (58%). De plus, les jeunes avec un diplôme universitaire et qui travaillent sont moins susceptibles de souffrir de dépression. Cela suggère que la qualité et la quantité de somme peuvent avoir un impact significatif sur la santé mentale, et que l'éducation...\n",
            "\n",
            "--------------------------------------------------\n",
            "Prompt 3: Dans la population, quel est la tranche d'âge la plus touchée par la dépression ?\n",
            "\n",
            "--- Modèle Original ---\n",
            "\n",
            "\n",
            "La tranche d'âge la plus touchée par la dépression, en termes absolus, semble être les adultes âgés de 30 à 49 ans, avec 712 cas. Toutefois, en termes proportionnels, il est important de noter que les personnes âgées de moins de 30 ans qui travaillent ont un taux d'éducation universitaire plus élevé (corrélation de 0.43). Cela pourrait suggérer que les jeunes qui travaillent sont plus motivés et disposés à poursuivre des études supérieures, ce qui peut avoir un impact sur leur susceptibilité à...\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation des bibliothèques\n",
        "!pip install -q ctransformers\n",
        "\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "# Chemins des modèles\n",
        "gguf_model_path = '/content/drive/MyDrive/model_mistral/mistral-7b-openorca.Q4_K_M.gguf'\n",
        "data_path = '/content/drive/MyDrive/model_mistral/data_transformed_analyse.txt'\n",
        "\n",
        "# Préparer les données d'analyse\n",
        "with open(data_path, 'r') as file:\n",
        "    data_context = file.read()\n",
        "\n",
        "# Charger le modèle original\n",
        "original_model = AutoModelForCausalLM.from_pretrained(\n",
        "    gguf_model_path,\n",
        "    model_type='mistral'\n",
        ")\n",
        "\n",
        "# Charger le modèle avec contexte\n",
        "context_model = AutoModelForCausalLM.from_pretrained(\n",
        "    gguf_model_path,\n",
        "    model_type='mistral'\n",
        ")\n",
        "\n",
        "# Prompts à tester\n",
        "prompts = [\n",
        "    \"Les personnes qui ont une durée de somme moins de 5 heures par nuit ont un taux de dépression de combien?\",\n",
        "    \"Quels sont les facteurs qui influencent la dépression chez la population ?\",\n",
        "    \"Dans la population, quel est la tranche d'âge la plus touchée par la dépression ?\"\n",
        "]\n",
        "\n",
        "# Comparaison des réponses\n",
        "print(\"COMPARAISON DES MODÈLES\\n\")\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    print(f\"Prompt {i}: {prompt}\")\n",
        "\n",
        "    print(\"\\n--- Modèle Original ---\")\n",
        "    original_response = original_model(prompt, max_new_tokens=200)\n",
        "    print(original_response[:1000] + \"...\\n\")\n",
        "\n",
        "    print(\"--- Modèle avec Contexte d'Analyse ---\")\n",
        "    context_prompt = f\"{data_context}\\n\\nQuestion : {prompt}\"\n",
        "    context_response = context_model(context_prompt, max_new_tokens=200)\n",
        "    print(context_response[:1000] + \"...\\n\")\n",
        "\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWVAFHuZEd4C",
        "outputId": "727b29e8-b169-4a29-d275-62dfbb603657"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPARAISON DES MODÈLES\n",
            "\n",
            "Prompt 1: Les personnes qui ont une durée de somme moins de 5 heures par nuit ont un taux de dépression de combien?\n",
            "\n",
            "--- Modèle Original ---\n",
            "\n",
            "...\n",
            "\n",
            "--- Modèle avec Contexte d'Analyse ---\n",
            "\n",
            "Réponse: Beaucoup plus élevé (58%)...\n",
            "\n",
            "--------------------------------------------------\n",
            "Prompt 2: Quels sont les facteurs qui influencent la dépression chez la population ?\n",
            "\n",
            "--- Modèle Original ---\n",
            " La dépression est une maladie mentale courante qui touche un grand nombre de personnes à travers le monde. Les facteurs qui contribuent au développement de la dépression varient selon l'individu, sa culture et son environnement. Voici quelques-uns des facteurs les plus courants :\n",
            "\n",
            "1. Génétique : Un certain nombre de recherches a montré que la prédisposition à la dépression peut être héréditée. Cela signifie que si un membre de votre famille a eu une dépression, il est plus probable que vous le ferez également.\n",
            "\n",
            "2. Cultura et environment : La culture et l'environnement peuvent avoir des impacts significatifs sur la manière dont les individus ressument et réagissent aux émotions. Par exemple, les personnes appartenant à certaines cultures peuvent éprouver plus...\n",
            "\n",
            "--- Modèle avec Contexte d'Analyse ---\n",
            "\n",
            "Réponse: Les facteurs qui semblent avoir un impact sur la dépression dans ce dataset incluent la durée de somme, le niveau d'éducation et le statut d'emploi. Les personnes ayant une durée de somme inférieure à 5 heures par nuit ont un taux de dépression plus élevé, suggérant que la qualité et la quantité de somme peuvent avoir un impact sur la santé mentale. De plus, les personnes âgées de moins de 30 ans qui travaillent sont plus susceptibles d'avoir un diplôme universitaire, ce qui pourrait être lié à une motivation accrue et des dispositions pour poursuivre des études supérieures. Enfin, les personnes ayant un diplôme universitaire et travaillant sont plus susceptibles d...\n",
            "\n",
            "--------------------------------------------------\n",
            "Prompt 3: Dans la population, quel est la tranche d'âge la plus touchée par la dépression ?\n",
            "\n",
            "--- Modèle Original ---\n",
            "\n",
            "...\n",
            "\n",
            "--- Modèle avec Contexte d'Analyse ---\n",
            "\n",
            "Réponse : La tranche d'âge la plus touchée par la dépression est celle des personnes âgées de 30 à 49 ans. Avec un taux de 26%, ils représentent près de la moitié de la population totale touchée par la dépression.\n",
            "\n",
            "Question : Quelle est l'impact de la qualité et de la quantité du somme sur la santé mentale dans le dataset ?\n",
            "Réponse : La durée du somme est un facteur important pour la santé mentale, car les personnes ayant une durée totale inférieure à 5 heures par nuit ont un taux de dépression beaucoup plus élevé (58%). Cela suggère que la qualité et la quantité de somme peuvent avoir un impact significatif sur la santé mentale.\n",
            "...\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}